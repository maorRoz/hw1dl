{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import keras\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../input/labels.csv')\n",
    "df_test = pd.read_csv('../input/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_series = pd.Series(df_train['breed'])\n",
    "one_hot = pd.get_dummies(targets_series, sparse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_labels = np.asarray(one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_resize_images(df,nrow=224,ncol=224,channels=3):\n",
    "    from scipy import misc\n",
    "    i = 0\n",
    "    x = []\n",
    "    y = []\n",
    "    for f, breed in tqdm(df.values):\n",
    "        img = misc.imread('../input/train/{}.jpg'.format(f))\n",
    "        label = one_hot_labels[i]\n",
    "        x.append(misc.imresize(img, (nrow, ncol,channels)))\n",
    "        y.append(label)\n",
    "        i += 1\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                        | 0/10222 [00:00<?, ?it/s]C:\\Users\\user\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:7: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  import sys\n",
      "C:\\Users\\user\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:9: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "  if __name__ == '__main__':\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 10222/10222 [02:00<00:00, 85.17it/s]\n"
     ]
    }
   ],
   "source": [
    "x_train,y_train = read_and_resize_images(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_resize_images_test(df,nrow=224,ncol=224,channels=3):\n",
    "    from scipy import misc\n",
    "    x = []\n",
    "    for f in tqdm(df['id'].values):\n",
    "        img = misc.imread('../input/test/{}.jpg'.format(f))\n",
    "        x.append(misc.imresize(img, (nrow, ncol,channels)))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                        | 0/10357 [00:00<?, ?it/s]C:\\Users\\user\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  \"\"\"\n",
      "C:\\Users\\user\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "  \n",
      "100%|████████████████████████████████████████████████████████████████████████████| 10357/10357 [02:59<00:00, 57.57it/s]\n"
     ]
    }
   ],
   "source": [
    "x_test = read_and_resize_images_test(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_raw = np.array(y_train, np.uint8)\n",
    "x_train_raw = np.array(x_train, np.float32) / 255.\n",
    "x_test  = np.array(x_test, np.float32) / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10222, 224, 224, 3)\n",
      "(10222, 120)\n",
      "(10357, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "print(x_train_raw.shape)\n",
    "print(y_train_raw.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_class = y_train_raw.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, Y_train, Y_valid = train_test_split(x_train_raw, y_train_raw, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv4 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv4 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv4 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 120)               3010680   \n",
      "=================================================================\n",
      "Total params: 23,035,064\n",
      "Trainable params: 3,010,680\n",
      "Non-trainable params: 20,024,384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_model = VGG19(#weights='imagenet',\n",
    "    weights = None, include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Add a new top layer\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "predictions = Dense(num_class, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# First: train only the top layers (which were randomly initialized)\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "callbacks_list = [keras.callbacks.EarlyStopping(monitor='val_acc', patience=3, verbose=1)]\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7155 samples, validate on 3067 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6496/7155 [==========================>...] - ETA: 4:17:49 - loss: 4.7874 - acc: 0.0000e+ - ETA: 2:09:40 - loss: 4.7877 - acc: 0.0000e+ - ETA: 1:26:57 - loss: 4.7871 - acc: 0.0000e+ - ETA: 1:05:34 - loss: 4.7872 - acc: 0.0000e+ - ETA: 52:44 - loss: 4.7880 - acc: 0.0000e+00 - ETA: 44:11 - loss: 4.7885 - acc: 0.0000e+ - ETA: 38:04 - loss: 4.7885 - acc: 0.0045   - ETA: 33:28 - loss: 4.7882 - acc: 0.00 - ETA: 29:54 - loss: 4.7871 - acc: 0.00 - ETA: 27:02 - loss: 4.7867 - acc: 0.00 - ETA: 24:41 - loss: 4.7872 - acc: 0.00 - ETA: 22:44 - loss: 4.7873 - acc: 0.00 - ETA: 21:05 - loss: 4.7871 - acc: 0.00 - ETA: 19:39 - loss: 4.7870 - acc: 0.00 - ETA: 18:25 - loss: 4.7865 - acc: 0.00 - ETA: 17:21 - loss: 4.7862 - acc: 0.00 - ETA: 16:23 - loss: 4.7859 - acc: 0.00 - ETA: 15:32 - loss: 4.7864 - acc: 0.00 - ETA: 14:47 - loss: 4.7858 - acc: 0.00 - ETA: 14:06 - loss: 4.7860 - acc: 0.00 - ETA: 13:28 - loss: 4.7861 - acc: 0.00 - ETA: 12:54 - loss: 4.7863 - acc: 0.00 - ETA: 12:23 - loss: 4.7867 - acc: 0.00 - ETA: 11:55 - loss: 4.7875 - acc: 0.00 - ETA: 11:28 - loss: 4.7875 - acc: 0.00 - ETA: 11:04 - loss: 4.7875 - acc: 0.00 - ETA: 10:41 - loss: 4.7875 - acc: 0.00 - ETA: 10:20 - loss: 4.7874 - acc: 0.00 - ETA: 10:00 - loss: 4.7873 - acc: 0.00 - ETA: 9:42 - loss: 4.7869 - acc: 0.0073 - ETA: 9:25 - loss: 4.7869 - acc: 0.007 - ETA: 9:08 - loss: 4.7873 - acc: 0.006 - ETA: 8:53 - loss: 4.7870 - acc: 0.006 - ETA: 8:39 - loss: 4.7871 - acc: 0.007 - ETA: 8:25 - loss: 4.7867 - acc: 0.007 - ETA: 8:12 - loss: 4.7863 - acc: 0.007 - ETA: 8:00 - loss: 4.7866 - acc: 0.008 - ETA: 7:49 - loss: 4.7866 - acc: 0.009 - ETA: 7:38 - loss: 4.7870 - acc: 0.008 - ETA: 7:27 - loss: 4.7873 - acc: 0.008 - ETA: 7:17 - loss: 4.7874 - acc: 0.008 - ETA: 7:07 - loss: 4.7870 - acc: 0.008 - ETA: 6:58 - loss: 4.7868 - acc: 0.008 - ETA: 6:49 - loss: 4.7865 - acc: 0.008 - ETA: 6:41 - loss: 4.7863 - acc: 0.009 - ETA: 6:33 - loss: 4.7860 - acc: 0.010 - ETA: 6:25 - loss: 4.7858 - acc: 0.010 - ETA: 6:17 - loss: 4.7858 - acc: 0.010 - ETA: 6:10 - loss: 4.7861 - acc: 0.010 - ETA: 6:03 - loss: 4.7861 - acc: 0.011 - ETA: 5:56 - loss: 4.7860 - acc: 0.012 - ETA: 5:50 - loss: 4.7859 - acc: 0.012 - ETA: 5:43 - loss: 4.7862 - acc: 0.011 - ETA: 5:37 - loss: 4.7864 - acc: 0.011 - ETA: 5:31 - loss: 4.7865 - acc: 0.011 - ETA: 5:26 - loss: 4.7865 - acc: 0.011 - ETA: 5:20 - loss: 4.7868 - acc: 0.011 - ETA: 5:15 - loss: 4.7871 - acc: 0.010 - ETA: 5:10 - loss: 4.7876 - acc: 0.010 - ETA: 5:05 - loss: 4.7874 - acc: 0.010 - ETA: 5:00 - loss: 4.7870 - acc: 0.010 - ETA: 4:55 - loss: 4.7871 - acc: 0.010 - ETA: 4:50 - loss: 4.7870 - acc: 0.010 - ETA: 4:46 - loss: 4.7867 - acc: 0.010 - ETA: 4:41 - loss: 4.7866 - acc: 0.010 - ETA: 4:37 - loss: 4.7866 - acc: 0.010 - ETA: 4:33 - loss: 4.7867 - acc: 0.010 - ETA: 4:29 - loss: 4.7866 - acc: 0.010 - ETA: 4:25 - loss: 4.7868 - acc: 0.010 - ETA: 4:21 - loss: 4.7868 - acc: 0.010 - ETA: 4:17 - loss: 4.7868 - acc: 0.010 - ETA: 4:14 - loss: 4.7866 - acc: 0.010 - ETA: 4:10 - loss: 4.7865 - acc: 0.010 - ETA: 4:06 - loss: 4.7865 - acc: 0.011 - ETA: 4:03 - loss: 4.7864 - acc: 0.010 - ETA: 4:00 - loss: 4.7864 - acc: 0.010 - ETA: 3:57 - loss: 4.7864 - acc: 0.010 - ETA: 3:54 - loss: 4.7864 - acc: 0.010 - ETA: 3:51 - loss: 4.7865 - acc: 0.010 - ETA: 3:48 - loss: 4.7863 - acc: 0.010 - ETA: 3:45 - loss: 4.7867 - acc: 0.010 - ETA: 3:41 - loss: 4.7864 - acc: 0.010 - ETA: 3:38 - loss: 4.7865 - acc: 0.010 - ETA: 3:35 - loss: 4.7865 - acc: 0.010 - ETA: 3:33 - loss: 4.7866 - acc: 0.010 - ETA: 3:30 - loss: 4.7869 - acc: 0.010 - ETA: 3:27 - loss: 4.7868 - acc: 0.010 - ETA: 3:24 - loss: 4.7868 - acc: 0.010 - ETA: 3:22 - loss: 4.7869 - acc: 0.010 - ETA: 3:19 - loss: 4.7870 - acc: 0.010 - ETA: 3:17 - loss: 4.7871 - acc: 0.010 - ETA: 3:14 - loss: 4.7871 - acc: 0.010 - ETA: 3:12 - loss: 4.7873 - acc: 0.010 - ETA: 3:09 - loss: 4.7871 - acc: 0.010 - ETA: 3:07 - loss: 4.7871 - acc: 0.010 - ETA: 3:04 - loss: 4.7869 - acc: 0.010 - ETA: 3:02 - loss: 4.7870 - acc: 0.010 - ETA: 3:00 - loss: 4.7870 - acc: 0.010 - ETA: 2:57 - loss: 4.7870 - acc: 0.010 - ETA: 2:55 - loss: 4.7872 - acc: 0.010 - ETA: 2:53 - loss: 4.7872 - acc: 0.010 - ETA: 2:50 - loss: 4.7873 - acc: 0.010 - ETA: 2:48 - loss: 4.7872 - acc: 0.010 - ETA: 2:46 - loss: 4.7871 - acc: 0.009 - ETA: 2:44 - loss: 4.7873 - acc: 0.009 - ETA: 2:42 - loss: 4.7872 - acc: 0.009 - ETA: 2:40 - loss: 4.7871 - acc: 0.009 - ETA: 2:38 - loss: 4.7874 - acc: 0.009 - ETA: 2:36 - loss: 4.7873 - acc: 0.009 - ETA: 2:34 - loss: 4.7873 - acc: 0.009 - ETA: 2:32 - loss: 4.7874 - acc: 0.009 - ETA: 2:30 - loss: 4.7873 - acc: 0.009 - ETA: 2:28 - loss: 4.7873 - acc: 0.009 - ETA: 2:26 - loss: 4.7875 - acc: 0.009 - ETA: 2:24 - loss: 4.7873 - acc: 0.009 - ETA: 2:22 - loss: 4.7871 - acc: 0.009 - ETA: 2:20 - loss: 4.7870 - acc: 0.009 - ETA: 2:19 - loss: 4.7869 - acc: 0.009 - ETA: 2:17 - loss: 4.7870 - acc: 0.009 - ETA: 2:15 - loss: 4.7871 - acc: 0.009 - ETA: 2:13 - loss: 4.7871 - acc: 0.009 - ETA: 2:11 - loss: 4.7872 - acc: 0.009 - ETA: 2:10 - loss: 4.7871 - acc: 0.009 - ETA: 2:08 - loss: 4.7870 - acc: 0.009 - ETA: 2:06 - loss: 4.7872 - acc: 0.009 - ETA: 2:05 - loss: 4.7871 - acc: 0.009 - ETA: 2:03 - loss: 4.7872 - acc: 0.009 - ETA: 2:01 - loss: 4.7871 - acc: 0.009 - ETA: 2:00 - loss: 4.7872 - acc: 0.009 - ETA: 1:58 - loss: 4.7871 - acc: 0.009 - ETA: 1:56 - loss: 4.7872 - acc: 0.009 - ETA: 1:55 - loss: 4.7872 - acc: 0.009 - ETA: 1:53 - loss: 4.7871 - acc: 0.009 - ETA: 1:52 - loss: 4.7870 - acc: 0.009 - ETA: 1:50 - loss: 4.7872 - acc: 0.009 - ETA: 1:49 - loss: 4.7871 - acc: 0.009 - ETA: 1:47 - loss: 4.7870 - acc: 0.009 - ETA: 1:46 - loss: 4.7869 - acc: 0.009 - ETA: 1:44 - loss: 4.7866 - acc: 0.009 - ETA: 1:43 - loss: 4.7864 - acc: 0.009 - ETA: 1:41 - loss: 4.7865 - acc: 0.009 - ETA: 1:40 - loss: 4.7864 - acc: 0.009 - ETA: 1:38 - loss: 4.7864 - acc: 0.009 - ETA: 1:37 - loss: 4.7865 - acc: 0.009 - ETA: 1:35 - loss: 4.7864 - acc: 0.009 - ETA: 1:34 - loss: 4.7865 - acc: 0.009 - ETA: 1:33 - loss: 4.7865 - acc: 0.009 - ETA: 1:31 - loss: 4.7863 - acc: 0.009 - ETA: 1:30 - loss: 4.7863 - acc: 0.009 - ETA: 1:28 - loss: 4.7861 - acc: 0.009 - ETA: 1:27 - loss: 4.7861 - acc: 0.009 - ETA: 1:26 - loss: 4.7860 - acc: 0.009 - ETA: 1:24 - loss: 4.7859 - acc: 0.009 - ETA: 1:23 - loss: 4.7858 - acc: 0.009 - ETA: 1:22 - loss: 4.7858 - acc: 0.009 - ETA: 1:20 - loss: 4.7856 - acc: 0.009 - ETA: 1:19 - loss: 4.7857 - acc: 0.009 - ETA: 1:17 - loss: 4.7857 - acc: 0.009 - ETA: 1:16 - loss: 4.7857 - acc: 0.009 - ETA: 1:15 - loss: 4.7857 - acc: 0.009 - ETA: 1:14 - loss: 4.7857 - acc: 0.008 - ETA: 1:12 - loss: 4.7858 - acc: 0.009 - ETA: 1:11 - loss: 4.7857 - acc: 0.009 - ETA: 1:10 - loss: 4.7858 - acc: 0.009 - ETA: 1:08 - loss: 4.7857 - acc: 0.008 - ETA: 1:07 - loss: 4.7857 - acc: 0.008 - ETA: 1:06 - loss: 4.7857 - acc: 0.009 - ETA: 1:05 - loss: 4.7857 - acc: 0.008 - ETA: 1:03 - loss: 4.7856 - acc: 0.009 - ETA: 1:02 - loss: 4.7858 - acc: 0.009 - ETA: 1:01 - loss: 4.7857 - acc: 0.009 - ETA: 1:00 - loss: 4.7858 - acc: 0.009 - ETA: 59s - loss: 4.7859 - acc: 0.009 - ETA: 57s - loss: 4.7858 - acc: 0.00 - ETA: 56s - loss: 4.7857 - acc: 0.00 - ETA: 55s - loss: 4.7858 - acc: 0.00 - ETA: 53s - loss: 4.7858 - acc: 0.00 - ETA: 52s - loss: 4.7857 - acc: 0.00 - ETA: 51s - loss: 4.7857 - acc: 0.00 - ETA: 50s - loss: 4.7857 - acc: 0.00 - ETA: 49s - loss: 4.7857 - acc: 0.00 - ETA: 47s - loss: 4.7856 - acc: 0.00 - ETA: 46s - loss: 4.7857 - acc: 0.00 - ETA: 45s - loss: 4.7858 - acc: 0.00 - ETA: 44s - loss: 4.7858 - acc: 0.00 - ETA: 42s - loss: 4.7858 - acc: 0.00 - ETA: 41s - loss: 4.7858 - acc: 0.00 - ETA: 40s - loss: 4.7859 - acc: 0.00 - ETA: 39s - loss: 4.7861 - acc: 0.00 - ETA: 38s - loss: 4.7861 - acc: 0.00 - ETA: 36s - loss: 4.7861 - acc: 0.00 - ETA: 35s - loss: 4.7861 - acc: 0.00 - ETA: 34s - loss: 4.7863 - acc: 0.00 - ETA: 33s - loss: 4.7862 - acc: 0.00 - ETA: 32s - loss: 4.7861 - acc: 0.00 - ETA: 31s - loss: 4.7863 - acc: 0.00 - ETA: 29s - loss: 4.7863 - acc: 0.00 - ETA: 28s - loss: 4.7864 - acc: 0.00 - ETA: 27s - loss: 4.7864 - acc: 0.00 - ETA: 26s - loss: 4.7865 - acc: 0.00 - ETA: 25s - loss: 4.7864 - acc: 0.00 - ETA: 24s - loss: 4.7864 - acc: 0.00 - ETA: 22s - loss: 4.7865 - acc: 0.0097\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7155/7155 [==============================] - ETA: 21s - loss: 4.7865 - acc: 0.00 - ETA: 20s - loss: 4.7864 - acc: 0.01 - ETA: 19s - loss: 4.7863 - acc: 0.01 - ETA: 18s - loss: 4.7863 - acc: 0.01 - ETA: 17s - loss: 4.7864 - acc: 0.01 - ETA: 16s - loss: 4.7864 - acc: 0.01 - ETA: 15s - loss: 4.7863 - acc: 0.01 - ETA: 13s - loss: 4.7862 - acc: 0.01 - ETA: 12s - loss: 4.7862 - acc: 0.01 - ETA: 11s - loss: 4.7863 - acc: 0.01 - ETA: 10s - loss: 4.7862 - acc: 0.01 - ETA: 9s - loss: 4.7862 - acc: 0.0100 - ETA: 8s - loss: 4.7861 - acc: 0.010 - ETA: 7s - loss: 4.7861 - acc: 0.010 - ETA: 6s - loss: 4.7860 - acc: 0.010 - ETA: 5s - loss: 4.7860 - acc: 0.010 - ETA: 3s - loss: 4.7861 - acc: 0.010 - ETA: 2s - loss: 4.7861 - acc: 0.010 - ETA: 1s - loss: 4.7862 - acc: 0.010 - ETA: 0s - loss: 4.7861 - acc: 0.010 - 340s 48ms/step - loss: 4.7861 - acc: 0.0101 - val_loss: 4.7804 - val_acc: 0.0134\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x27f97d03ac8>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, epochs=1, validation_data=(X_valid, Y_valid), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10357/10357 [==============================] - ETA: 45:0 - ETA: 24:3 - ETA: 17:4 - ETA: 14:1 - ETA: 12:1 - ETA: 10:5 - ETA: 9:5 - ETA: 9: - ETA: 8: - ETA: 8: - ETA: 7: - ETA: 7: - ETA: 7: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 59s - ETA: 58 - ETA: 57 - ETA: 56 - ETA: 55 - ETA: 54 - ETA: 53 - ETA: 52 - ETA: 51 - ETA: 51 - ETA: 50 - ETA: 49 - ETA: 48 - ETA: 47 - ETA: 46 - ETA: 45 - ETA: 44 - ETA: 43 - ETA: 42 - ETA: 41 - ETA: 40 - ETA: 39 - ETA: 38 - ETA: 37 - ETA: 36 - ETA: 36 - ETA: 35 - ETA: 34 - ETA: 33 - ETA: 32 - ETA: 31 - ETA: 30 - ETA: 29 - ETA: 28 - ETA: 28 - ETA: 27 - ETA: 26 - ETA: 25 - ETA: 24 - ETA: 23 - ETA: 22 - ETA: 21 - ETA: 20 - ETA: 19 - ETA: 18 - ETA: 17 - ETA: 16 - ETA: 16 - ETA: 15 - ETA: 14 - ETA: 13 - ETA: 12 - ETA: 11 - ETA: 10 - ETA: 9 - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 298s 29ms/step\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(x_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>affenpinscher</th>\n",
       "      <th>afghan_hound</th>\n",
       "      <th>african_hunting_dog</th>\n",
       "      <th>airedale</th>\n",
       "      <th>american_staffordshire_terrier</th>\n",
       "      <th>appenzeller</th>\n",
       "      <th>australian_terrier</th>\n",
       "      <th>basenji</th>\n",
       "      <th>basset</th>\n",
       "      <th>...</th>\n",
       "      <th>toy_poodle</th>\n",
       "      <th>toy_terrier</th>\n",
       "      <th>vizsla</th>\n",
       "      <th>walker_hound</th>\n",
       "      <th>weimaraner</th>\n",
       "      <th>welsh_springer_spaniel</th>\n",
       "      <th>west_highland_white_terrier</th>\n",
       "      <th>whippet</th>\n",
       "      <th>wire-haired_fox_terrier</th>\n",
       "      <th>yorkshire_terrier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000621fb3cbb32d8935728e48679680e</td>\n",
       "      <td>0.008050</td>\n",
       "      <td>0.009947</td>\n",
       "      <td>0.008731</td>\n",
       "      <td>0.008837</td>\n",
       "      <td>0.007782</td>\n",
       "      <td>0.007521</td>\n",
       "      <td>0.009322</td>\n",
       "      <td>0.009846</td>\n",
       "      <td>0.008070</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007892</td>\n",
       "      <td>0.007811</td>\n",
       "      <td>0.007052</td>\n",
       "      <td>0.007680</td>\n",
       "      <td>0.008373</td>\n",
       "      <td>0.008222</td>\n",
       "      <td>0.007906</td>\n",
       "      <td>0.008442</td>\n",
       "      <td>0.007829</td>\n",
       "      <td>0.007658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00102ee9d8eb90812350685311fe5890</td>\n",
       "      <td>0.007917</td>\n",
       "      <td>0.010251</td>\n",
       "      <td>0.008766</td>\n",
       "      <td>0.008906</td>\n",
       "      <td>0.007710</td>\n",
       "      <td>0.007367</td>\n",
       "      <td>0.009454</td>\n",
       "      <td>0.010038</td>\n",
       "      <td>0.008031</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007867</td>\n",
       "      <td>0.007679</td>\n",
       "      <td>0.006871</td>\n",
       "      <td>0.007595</td>\n",
       "      <td>0.008436</td>\n",
       "      <td>0.008229</td>\n",
       "      <td>0.007926</td>\n",
       "      <td>0.008456</td>\n",
       "      <td>0.007753</td>\n",
       "      <td>0.007598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0012a730dfa437f5f3613fb75efcd4ce</td>\n",
       "      <td>0.008029</td>\n",
       "      <td>0.009849</td>\n",
       "      <td>0.008694</td>\n",
       "      <td>0.008789</td>\n",
       "      <td>0.007842</td>\n",
       "      <td>0.007619</td>\n",
       "      <td>0.009269</td>\n",
       "      <td>0.009753</td>\n",
       "      <td>0.008102</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007926</td>\n",
       "      <td>0.007824</td>\n",
       "      <td>0.007121</td>\n",
       "      <td>0.007721</td>\n",
       "      <td>0.008360</td>\n",
       "      <td>0.008246</td>\n",
       "      <td>0.007971</td>\n",
       "      <td>0.008424</td>\n",
       "      <td>0.007826</td>\n",
       "      <td>0.007699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>001510bc8570bbeee98c8d80c8a95ec1</td>\n",
       "      <td>0.007976</td>\n",
       "      <td>0.010337</td>\n",
       "      <td>0.008740</td>\n",
       "      <td>0.008917</td>\n",
       "      <td>0.007741</td>\n",
       "      <td>0.007405</td>\n",
       "      <td>0.009515</td>\n",
       "      <td>0.010022</td>\n",
       "      <td>0.007930</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007893</td>\n",
       "      <td>0.007553</td>\n",
       "      <td>0.006838</td>\n",
       "      <td>0.007592</td>\n",
       "      <td>0.008421</td>\n",
       "      <td>0.008200</td>\n",
       "      <td>0.007841</td>\n",
       "      <td>0.008407</td>\n",
       "      <td>0.007621</td>\n",
       "      <td>0.007562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>001a5f3114548acdefa3d4da05474c2e</td>\n",
       "      <td>0.008043</td>\n",
       "      <td>0.010079</td>\n",
       "      <td>0.008693</td>\n",
       "      <td>0.008868</td>\n",
       "      <td>0.007804</td>\n",
       "      <td>0.007516</td>\n",
       "      <td>0.009406</td>\n",
       "      <td>0.009850</td>\n",
       "      <td>0.008018</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007929</td>\n",
       "      <td>0.007652</td>\n",
       "      <td>0.006996</td>\n",
       "      <td>0.007646</td>\n",
       "      <td>0.008404</td>\n",
       "      <td>0.008233</td>\n",
       "      <td>0.007875</td>\n",
       "      <td>0.008398</td>\n",
       "      <td>0.007724</td>\n",
       "      <td>0.007655</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 121 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id  affenpinscher  afghan_hound  \\\n",
       "0  000621fb3cbb32d8935728e48679680e       0.008050      0.009947   \n",
       "1  00102ee9d8eb90812350685311fe5890       0.007917      0.010251   \n",
       "2  0012a730dfa437f5f3613fb75efcd4ce       0.008029      0.009849   \n",
       "3  001510bc8570bbeee98c8d80c8a95ec1       0.007976      0.010337   \n",
       "4  001a5f3114548acdefa3d4da05474c2e       0.008043      0.010079   \n",
       "\n",
       "   african_hunting_dog  airedale  american_staffordshire_terrier  appenzeller  \\\n",
       "0             0.008731  0.008837                        0.007782     0.007521   \n",
       "1             0.008766  0.008906                        0.007710     0.007367   \n",
       "2             0.008694  0.008789                        0.007842     0.007619   \n",
       "3             0.008740  0.008917                        0.007741     0.007405   \n",
       "4             0.008693  0.008868                        0.007804     0.007516   \n",
       "\n",
       "   australian_terrier   basenji    basset        ...          toy_poodle  \\\n",
       "0            0.009322  0.009846  0.008070        ...            0.007892   \n",
       "1            0.009454  0.010038  0.008031        ...            0.007867   \n",
       "2            0.009269  0.009753  0.008102        ...            0.007926   \n",
       "3            0.009515  0.010022  0.007930        ...            0.007893   \n",
       "4            0.009406  0.009850  0.008018        ...            0.007929   \n",
       "\n",
       "   toy_terrier    vizsla  walker_hound  weimaraner  welsh_springer_spaniel  \\\n",
       "0     0.007811  0.007052      0.007680    0.008373                0.008222   \n",
       "1     0.007679  0.006871      0.007595    0.008436                0.008229   \n",
       "2     0.007824  0.007121      0.007721    0.008360                0.008246   \n",
       "3     0.007553  0.006838      0.007592    0.008421                0.008200   \n",
       "4     0.007652  0.006996      0.007646    0.008404                0.008233   \n",
       "\n",
       "   west_highland_white_terrier   whippet  wire-haired_fox_terrier  \\\n",
       "0                     0.007906  0.008442                 0.007829   \n",
       "1                     0.007926  0.008456                 0.007753   \n",
       "2                     0.007971  0.008424                 0.007826   \n",
       "3                     0.007841  0.008407                 0.007621   \n",
       "4                     0.007875  0.008398                 0.007724   \n",
       "\n",
       "   yorkshire_terrier  \n",
       "0           0.007658  \n",
       "1           0.007598  \n",
       "2           0.007699  \n",
       "3           0.007562  \n",
       "4           0.007655  \n",
       "\n",
       "[5 rows x 121 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub = pd.DataFrame(preds)\n",
    "col_names = one_hot.columns.values\n",
    "sub.columns = col_names\n",
    "# Insert the column id from the sample_submission at the start of the data frame\n",
    "sub.insert(0, 'id', df_test['id'])\n",
    "sub.head(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
